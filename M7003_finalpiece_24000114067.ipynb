{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M7003_finalpiece_24000114067\n",
    "\n",
    "LIS MASc  \n",
    "The Right Word  \n",
    "Final Piece (Choice 2: NLP)  \n",
    "Student number: 24000114067\n",
    "\n",
    "Access notebook, data, pdf and web app in the [GitHub repository](https://github.com/noah-art3mis/nlp-assignment).\n",
    "\n",
    "PDF generated using [nbconvert](https://nbconvert.readthedocs.io/en/latest/)\n",
    "\n",
    "## Intersect - Personalized job matching\n",
    "\n",
    "This is an web app (acess on [streamlit]()) that can be used to find jobs that match your profile.\n",
    "\n",
    "similar in concept to [marginalia](https://marginalia-search.com/) in that it helps users to find nonobvious results buried deep into the ordering given by a website.\n",
    "\n",
    "-   [ ] The NLP-aided analysis of a corpus of documents will extract non-obvious insights from a corpus of documents using tools from coding and data science. The corpus of documents could consist of a literary archive, a body of social media posts, a scrape from an online knowledge database; ideally, these would have some relation to the capstone problem. The submission should offer a graphical representation of its discoveries and include a write up of c. 500 words the documents the process involved. A link to an executable code notebook should also be included as part of the submission.\n",
    "-   [ ] Assessment Choice 2 should be submitted as a PDF file that contains a brief summary of the submission and which contains a link to a GitHUb repository where the code notebook and all relevant data can be found - max 500 words\n",
    "\n",
    "ai sells. while i find it intellectually dishonest to call reordering a table using embeddings 'intelligent', in the current zeitgeist i believe this is the play. i have seen far worse.\n",
    "\n",
    "-   findings\n",
    "-   illustrations\n",
    "\n",
    "this is not necessary related to the capstone but is relevant for my future and can also help other people and also be a piece for a portfolio. this can become an actual product if it is good.\n",
    "\n",
    "the data is publicly available and their robots.txt file does not disallow scraping of these pages.\n",
    "\n",
    "while we're here, we can also do some basic nlp to see if theres anything there. we already have the data.\n",
    "\n",
    "more details regarding the choices made can be found in the README file.\n",
    "\n",
    "## comparing the results from semantic to lexical search in a subjective and qualitative manner we get that X. for a more serious project this analysis would need to be automated with benchmarks and evals and so on.\n",
    "\n",
    "This work contains [ ] words (not counting tables and figures).\n",
    "\n",
    "---\n",
    "\n",
    "-   The submission should use technical concepts from linguistics and/or NLP\n",
    "    in practically useful way, drawing on relevant sources and tools.\n",
    "-   The submission should justify the intellectual choices it makes with respect\n",
    "    to analysis and/or optimisation in relation to theoretical methods and\n",
    "    established methodologiesâ€”and do so in a critically informed way.\n",
    "-   Where relevant, the submission should convincingly manipulate language\n",
    "    and/or narrative qualitatively and/or quantitatively, using appropriate\n",
    "    methods.\n",
    "-   The assessment should evidence skills in the communication of complex\n",
    "    ideas in its chosen format.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Explanations\n",
    "\n",
    "-   the idea is to intersect two searches - the original one with keywords and the semantic one with text. this makes it so you can search both for what you want and what you need at the same time. you can do keyword as a want (`fun`) or as need (`data`) and vice versa.\n",
    "-   OpenAI, Streamlit and Cohere are the fastest ways to make a prototype. This is the same reason there is no need for a vector database or even sqlite.\n",
    "-   For similarity we can use a dot product because OpenAI embeddings are normalized.\n",
    "-   _CV-Library_ is used as the data source as it is cited by the [UK Government](https://nationalcareers.service.gov.uk/careers-advice/advertised-job-vacancies).\n",
    "-   Scraping\n",
    "    -   Respects robots.txt\n",
    "    -   just a request doesnt work (403)\n",
    "    -   request with user agent doesnt work (403)\n",
    "    -   request with genuine headers doesnt work (403)\n",
    "    -   request with scrapeops fake headers doesnt work (403)\n",
    "    -   request with scrapeops proxy works but takes like 20s per request. this is tolerable for this application. scrapeops is free.\n",
    "-   test cvs generated by chatgpt with the prompt 'generate the perfect cv for this position' get around 0.65 similarity.\n",
    "-   BM25 is an altered version of TF-IDF that is standard in the industry.\n",
    "    -   https://www.elastic.co/blog/practical-bm25-part-2-the-bm25-algorithm-and-its-variables\n",
    "    -   https://docs.llamaindex.ai/en/stable/examples/retrievers/bm25_retriever/\n",
    "    -   bm25f can weight different parameters as relevant but at this point you are replicating the job board and that is out of scope.\n",
    "    -   the pdf/text input can be anything that you like. search for a job that is like a song or a poem or something. go nuts\n",
    "    -   note that the similarity values do not mean much of anything in absolute. see\n",
    "        -   https://datascience.stackexchange.com/questions/101862/cosine-similarity-between-sentence-embeddings-is-always-positive\n",
    "        -   https://vaibhavgarg1982.medium.com/why-are-cosine-similarities-of-text-embeddings-almost-always-positive-6bd31eaee4d5\n",
    "    -   much like llms, embeddings do averages. and so this is going to be generic. if you are trying to do something fancy, you might want to use this in a nonstandard way as well. such as using weird keywords in the search.\n",
    "    -   biggest issue time-wise is the free proxies. just buy good ones. also less of a security risk.\n",
    "    -   for a production-grade implementation of this we might want to take a look at https://github.com/AmenRa/retriv\n",
    "    -   Elastic search setup is a bit more involved so out of scope\n",
    "    -   why pca and kmeans\n",
    "\n",
    "bm25 is is an industry standard algorithm for search. bm25 itself is a variation on tfidf. this uses the lucene method as it is the default of the bm25s package.\n",
    "\n",
    "can also add a reranker after the semantic search\n",
    "\n",
    "-   cohere rerank\n",
    "-\n",
    "\n",
    "measure how much time it takes to use a reranker and ebedding local and remote.\n",
    "\n",
    "lexical search. sparse vectors\n",
    "semantic search. dense vectors.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
